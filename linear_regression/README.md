# Linear Regression

## Overview
This module demonstrates a basic linear regression workflow using Python. It generates synthetic data, splits it into training and testing sets, trains a `LinearRegression` model, evaluates its performance using Mean Squared Error (MSE) and R-squared, and visualizes the results.

## Installation
To run this script, you need to have the following Python libraries installed. You can install them using pip:

```bash
pip install pandas numpy scikit-learn matplotlib
```

## Usage
To use this module, simply run the Python script. It will generate data, train the model, print evaluation metrics, and display a plot of the regression line.

```bash
python your_script_name.py
```

Replace `your_script_name.py` with the actual name of your Python file.

**Example Output (console):**
```
Mean Squared Error: X.XX
R-squared: Y.YY
Model coefficients: A.AA
Model intercept: B.BB
```

A matplotlib plot window titled "Linear Regression Fit" will also appear, showing the actual data points and the fitted regression line.

## Inputs & Outputs

### Inputs
The script generates its own synthetic data internally and does not require any external input files or command-line arguments.

-   **Synthetic Feature (`X`)**: 100 random values between 0 and 100.
-   **Synthetic Target (`y`)**: Generated as `3 * X + noise`, where `noise` is a small random value.

### Outputs
-   **Console Output**:
    -   `Mean Squared Error`: A floating-point number indicating the average squared difference between actual and predicted values on the test set.
    -   `R-squared`: A floating-point number representing the proportion of the variance in the dependent variable that is predictable from the independent variable(s).
    -   `Model coefficients`: The learned slope of the regression line.
    -   `Model intercept`: The learned y-intercept of the regression line.
-   **Graphical Output**:
    -   A matplotlib plot showing:
        -   Scatter plot of `X_test` vs. `y_test` (actual data points).
        -   A red line representing the regression fit (`X_test` vs. `y_pred`).

## Explanation
The script follows a standard machine learning pipeline for linear regression:

1.  **Seed Setting**: `np.random.seed(42)` ensures reproducibility of the random data generation.
2.  **Data Generation**:
    -   A feature vector `X` is created using `np.random.rand` to produce 100 random numbers, scaled by 100.
    -   A target vector `y` is generated based on `X` with a linear relationship (`3 * X`) and some added random noise.
3.  **Data Splitting**:
    -   The `train_test_split` function from `sklearn.model_selection` divides the generated `X` and `y` into training (80%) and testing (20%) sets. This allows the model to be trained on one subset of data and evaluated on unseen data.
4.  **Model Initialization and Training**:
    -   An instance of `LinearRegression` from `sklearn.linear_model` is created.
    -   The `fit` method is called on the training data (`X_train`, `y_train`) to learn the optimal coefficients and intercept for the linear model.
5.  **Prediction**:
    -   The trained model's `predict` method is used on the test feature set (`X_test`) to generate predictions (`y_pred`).
6.  **Model Evaluation**:
    -   `mean_squared_error` calculates the MSE between the actual test values (`y_test`) and the predicted values (`y_pred`).
    -   `r2_score` calculates the R-squared value, providing a measure of how well the model's predictions explain the variance in the actual data.
7.  **Results Display**:
    -   The calculated MSE, R-squared, model coefficients, and intercept are printed to the console.
8.  **Visualization**:
    -   `matplotlib.pyplot` is used to create a scatter plot of the actual test data points and overlay the regression line generated by the model. This provides a visual representation of how well the model fits the data.

## License
This project is open-source and available under the [License Name] license. (e.g., MIT, Apache 2.0, etc.)